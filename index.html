<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<link rel="stylesheet" href="style.css">
		<title>Investigating Modality Contribution in Audio LLMs for Music</title>
	</head>

	<body class="body_main_page">
		<div class="header">
			<h1>Investigating Modality Contribution in Audio LLMs for Music</h1>
			<div class="row">
				<div class="column"><a href="https://giovana-morais.github.io"><h3>Giovana
							Morais<sup>1</sup></h3></a></div>
				<div class="column"><a href="https://magdalenafuentes.com"><h3>Magdalena
							Fuentes<sup>1,2</sup><h3></a></div>
			</div>
			</div>
			<div class="row">
				<div class="column"><sup>1</sup>: New York University, Music and Audio Research Lab</div>
			</div>
			<div class="row">
				<div class="column"><sup>2</sup>: New York University, Integrated Design and Media</div>
			</div>
			<br>
			<div class="row">
				<div class="column"><a href="https://arxiv.org/pdf/2509.20641"><h2>Paper</h2></a></div>
				<div class="column"><a href="https://github.com/giovana-morais/2025_investigating_mmshap"><h2>Code</h2></a></div>
			</div>
		</div>

		<div class="content">
			<hr>
			<details>
				<summary><h2>Abstract</h2></summary>
				<p>Audio Large Language Models (Audio LLMs) enable human-like conversation
				about music, yet it is unclear if they are truly listening to the audio or
				just using textual reasoning, as recent benchmarks suggest. This paper
				investigates this issue by quantifying the contribution of each modality to
				a model's output.  We adapt the MM-SHAP framework, a performance-agnostic
				score based on Shapley values that quantifies the relative contribution of
				each modality to a modelâ€™s prediction. We evaluate two models on the
				MuChoMusic benchmark and find that the model with higher accuracy relies
				more on text to answer questions, but further inspection shows that even if
				the overall audio contribution is low, models can successfully localize key
				sound events, suggesting that audio is not entirely ignored. Our study is
				the first application of MM-SHAP to Audio LLMs and we hope it will serve as
				a foundational step for future research in explainable AI and audio.</p>
			</details>

			<h2>Summary of the work</h2>

			<p>In this work, we investigate how Audio LLMs are using audio
			information to answer multiple-choice questions.
			An Audio LLM is a Large Language Model that is capable of
			processing audio data. These models usually receive as input, an audio file
			(.wav, .mp3 etc) and a text prompt. In theory, the prompt can be any text.

			We use the
			<a href="https://mulab-mir.github.io/muchomusic/">MuChoMusic benchmark</a>
			as our dataset.
			In this benchmark, the prompts are multiple-choice questions regarding the
			input audio.</p>

			<p>We'll now use a toy example to help:</p>

			<blockquote>
				<audio controls class="player">
					<source src="data/audio/toy_example.wav" type="audio/wav">
				</audio> <br><br>

				What is the sound that happen in the audio?<br>
				Options: (A) Crickets (B) Jackhammer (C) Birds chirping (D) Car horn<br>
				The correct answer is:
			</blockquote>

			<p>If you chose option (B) Jackhammer, you're correct and already doing
			better than a lot of models.</p>

			<p>For this, we first calculate
			<a href="https://christophm.github.io/interpretable-ml-book/shapley.html">Shapley
			values</a> for our audio and text features and use them to measure how each
		modality contributed to a given output. This method is called
			<a href="https://aclanthology.org/2023.acl-long.223/">MM-SHAP</a>.</p><br>

			<p>We test this approach with 2 models tested in the original MuChoMusic paper:
			<a href="https://github.com/QwenLM/Qwen-Audio">Qwen-Audio</a> and <a
				 href="https://github.com/ncsrsadhana/MULLaMA">MU-LLaMA</a>.</p>

			<h2>Interactive examples</h2>
			<p>We divide our examples in two categories: the ones in which the answer is a
			single-sounding event (which we inspected and annotated the ground truth)
			and randomly selected examples. For the random examples, we do not have the
			ground truth annotations as the answer usually required long-term
			understanding of the audio. As discussed in our paper, how to annotate those
			examples remains an open question.</p>

			<p>In the subsection below, we provide a brief explanation of the plot
			components. Feel free to skip it and play with the examples.</p>

			<details>
				<summary><h3>How to read the plots</h3></summary>

				<p>Each plot will have a similar structure to this one:
				<br>
				<img src="data/figs/placeholder.png"></img>
				<br>
				which has a lot of information to unpack.</p>

				<ul>
					<li><b>Experiment</b> name: this is a combination of the model + input type.
						So, in our example "Qwen-Audio MC-PI" we have the model
						<b>Qwen-Audio</b> and the experiment <b>Multiple-Choice with Previous
							Instructions</b>.</li>
					<li><b>Current view</b>: this is whether we are looking into the aggregate
						view, i.e. sum across output tokens, or the values for a single
						token.</li>
					<li><b>Stats</b>: a brief view of the actual values of the features. This view
					is optional.</li>
					<li><b>Question</b>: this is the prompt that the model receive. We display this
						information here as a <i>list of tokens</i> that the model uses based
						on the input. </li>
					<li><b>Model Answer</b>: this is the model output. Here is where the
						interactivit shines. You can click in whatever
						token you want to see how much both audio and the input text contributed to
						the generated token. If you want to go back to the original aggregated
						view, just click in the "Reset view" button.</li>
					<li><b>Waveform</b>: the input waveform. You can click in a timestamp on it to
						update the audio player. The playhead is shared among the plots.</li>
					<li><b>Absolute value</b>: This is the absolute value of this sample
						Shapley values. In practice, this is what is used to calculate the
						modality contribution score. It uses positive and negative values
						equally.</li>
					<li><b>Positive Only</b>: This subplot shows only the positive Shapley
						values, meaning that it highlights the features that contributed
						<b>positively</b> for the given output.</li>
					<li><b>Negative Only</b>: As before, this subplot shows only the
						negative Shapley values, so it shows what features contributed
						negatively to the output</li>
				</ul>
			</details>


			<h3>Single-sounding events</h3>
			<p>Those are events that the answer is very well localized in the sound. We
			refer again to our paper in case you want to see the methodology in
			details. To see the comparison across experiments, click in the question.</p>

			<div class="example_list" id="sse_list">
				<div class="example_item">
					<a href="comparison.html?qid=719">What sound effect can be heard in this
						piece?</a>
					<br>
					<audio controls class="player">
						<source src="data/audio/719.wav" type="audio/wav">
					</audio>
				</div>
				<div class="example_item">
					<a href="comparison.html?qid=427">Example 2 - QID 427</a>
					<br>
					<audio controls class="player">
						<source src="data/audio/719.wav" type="audio/wav">
					</audio>
				</div>
			</div>

			<br>
			<h3>Random Examples</h3>
			<p>Those were chosen randomly to show that the method is harder to
			interpret depending on the scenario. Here we have questions that the
			answer is not a single sound. In MuChoMusic, these questions are the
			majority. To see the comparison across experiments, click the question.</p>
			<br>
			<div class="example_list" id="random_list">
				<div class="example_item">
					<a href="comparison.html?qid=719">What sound effect can be heard in this
						piece?</a>
					<br>
					<audio controls class="player">
						<source src="data/audio/719.wav" type="audio/wav">
					</audio>
				</div>
				<div class="example_item">
					<a href="comparison.html?qid=427">Example 2 - QID 427</a>
					<br>
					<audio controls class="player">
						<source src="data/audio/719.wav" type="audio/wav">
					</audio>
				</div>
			</div>

			<h2>Citation</h2>
			<code id="citation">
				@misc{morais2025investigatingmodalitycontributionaudio,
					title={Investigating Modality Contribution in Audio LLMs for Music},
					author={Giovana Morais and Magdalena Fuentes},
					year={2025},
					eprint={2509.20641},
					archivePrefix={arXiv},
					primaryClass={cs.LG},
					url={https://arxiv.org/abs/2509.20641},
				}
			</code>

			<h2>Disclaimer</h2>

			<p>The visualizations were developed in
			<a href="https://d3js.org/">D3.js</a> with the support of
			<a href="https://gemini.google.com/app">Gemini</a>. I (Giovana) reviewed the code and tested it entirely, so any
			possible problems or inconsistencies are my responsability. Also, I'm a
			terrible designer and front-end developer. That is to say that the source
			code of this demo will 100% chaotic and probably get my computer science
			degree revoked. But, let's be honest, it looks super cool! If you
			disagree, please lie to me.
			If you find anything wrong, let me know by
			sending an email to giovana.morais@nyu.edu.<br>Obrigada!</p>
		</div>
		<script src="js/parse_question_list.js"></script>
	</body>
</html>
